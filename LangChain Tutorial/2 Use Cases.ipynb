{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc4930e5-a7d3-418f-ba05-c456df764846",
   "metadata": {},
   "source": [
    "## Use Cases\n",
    "Different end-to-end use cases that LangChain can help with. For each use case, we not only motivate the use case but also discuss which components are most helpful for solving that use case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97faa8a-d53c-4366-9056-331959e0c2fd",
   "metadata": {},
   "source": [
    "### Personal Assistants\n",
    "Personal assistants are a perfect application to build because they combine both of the core value props of LangChain (action taking and personalized data). In order to build a personal assistant you should understand the following concepts:\n",
    "1. `PromptTemplate` - this will guide how your personal assistant acts. Are they sassy? Helpful? These can be used to give your personal assistant some character.\n",
    "2. `Memory` - your personal assistant should probably remember things. They should definitely be able to hold a conversation (short term memory) and they should probably have some concept of long term memory as well.\n",
    "3. `Tools` - your personal assistant will be differentiated by the tools you give it. What should it know how to do?\n",
    "4. `Agent` - your personal assistant will have to understand what actions it should take. Constructing the best agent possible will be important.\n",
    "5. `Agent Executor` - after you've got your tools and your agent, in order to put it into practice you'll need to set up an environment for the agent to use those tools. This is where the Agent Executor comes into play."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d15367-294f-411b-bf1c-4eda00caf88d",
   "metadata": {},
   "source": [
    "### Question Answering Over Documents\n",
    "Although LLMs are powerful, they do not know about information they were not trained on. If you want to use an LLM to answer questions about documents it was not trained on, you have to give it information about those documents. The most common way to do this is through \"retrieval augmented generation\" (RAG).\n",
    "\n",
    "The idea of retrieval augmented generation is that when given a question you first do a retrieval step to fetch any relevant documents. You then pass those documents, along with the original question, to the language model and have it generate a response. In order to do this, however, you first have to have your documents in a format where they can be queried in such a manner. This page goes over the high level ideas between those two steps: (1) ingestion of documents into a queriable format, and then (2) the retrieval augmented generation chain.\n",
    "\n",
    "- **Ingestion** - In order use a language model to interact with your data, you first have to get in a suitable format. That format would be an Index. By putting data into an Index, you make it easy for any downstream steps to interact with it. There are several types of indexes, but by far the most common one is a Vectorstore. Ingesting documents into a vectorstore can be done with the following steps:\n",
    "  1. Load documents (using a Document Loader)\n",
    "  2. Split documents (using a Text Splitter)\n",
    "  3. Create embeddings for documents (using a Text Embedding Model)\n",
    "  4. Store documents and embeddings in a vectorstore\n",
    "\n",
    "- **Generation** - Now that we have an Index, how do we use this to do generation? This can be broken into the following steps:\n",
    "  1. Receive user question\n",
    "  2. Lookup documents in the index relevant to the question\n",
    "  3. Construct a PromptValue from the question and any relevant documents (using a PromptTemplate).\n",
    "  4. Pass the PromptValue to a model\n",
    "  5. Get back the result and return to the user."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266e821c-1986-48d9-bbdc-5ff28b2cfe28",
   "metadata": {},
   "source": [
    "### Chatbots (ChatGPT)\n",
    "ChatGPT took the world by storm by exposing a powerful language model with a new interface - chat. There are several components that go into building a chatbot.\n",
    "1. **The model** - you can construct a chatbot from a normal language model or a Chat Model. The important thing to remember is that even if you are using a Chat Model, the API itself is stateless, meaning it won't remember previous interactions - you have to pass them in.\n",
    "2. **PromptTemplate** - this will guide how your chatbot acts. Are they sassy? Helpful? These can be used to give your chatbot some character.\n",
    "3. **Memory** - as mentioned above, the models themselves are stateless. Memory brings some concept of state to the table, allowing it remember previous interactions\n",
    "\n",
    "Chatbots are often very powerful and more differentiated when combined with other sources of data. The same techniques that underpin \"Question Answering Over Docs\" can also be used here to give your chatbot access to that data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8acd65-f710-4162-bd32-fa7cb7617929",
   "metadata": {},
   "source": [
    "### Querying Tabular Data\n",
    "Lots of data and information is stored in tabular data, whether it be csvs, excel sheets, or SQL tables. This page covers all resources available in LangChain for working with data in this format.\n",
    "- **Document Loading** - If you have text data stored in a tabular format, you may want to load the data into a Document and then index it as you would other text/unstructured data. For this, you should use a document loader like the CSVLoader and then you should create an Index over that data, and query it that way.\n",
    "- **Querying** - If you have more numeric tabular data, or have a large amount of data and don't want to index it, you can also use a language model to interact with it directly.\n",
    "  - **Chains** - If you are just getting started, and you have relatively small/simple tabular data, you should get started with chains. Chains are a sequence of predetermined steps, so they are good to get started with as they give you more control and let you understand what is happening better.\n",
    "  - **Agents** - Agents are more complex, and involve multiple queries to the LLM to understand what to do. The downside of agents are that you have less control. The upside is that they are more powerful, which allows you to use them on larger databases and more complex schemas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fcebeb-3baa-4e5d-9f3e-fac72dbcfb9c",
   "metadata": {},
   "source": [
    "### Interacting with APIs\n",
    "APIs are powerful because they both allow you to take actions via them, but also they can allow you to query data through them. This page covers all resources available in LangChain for working with APIs.\n",
    "- **Chains** - If you are just getting started, and you have s relatively small/simple API, you should get started with chains. Chains are a sequence of predetermined steps, so they are good to get started with as they give you more control and let you understand what is happening better.\n",
    "- **Agents** - Agents are more complex, and involve multiple queries to the LLM to understand what to do. The downside of agents are that you have less control. The upside is that they are more powerful, which allows you to use them on larger or more complex APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbed977-576c-4ee8-8287-984af7208d80",
   "metadata": {},
   "source": [
    "### Extraction\n",
    "Language models are actually great at extracting structured information from unstructured text. This is useful because a lot of information is stored as text, but in order to make it most usable downstream it is often convinient to convert it to a structured format.\n",
    "\n",
    "The most useful concept to understand here is the idea of OutputParsers. OutputParsers are responsible for specifying the schema a language model should respond in, and then parsing their raw-text output into that structured format.\n",
    "\n",
    "The way you would use these to do extraction is that you would define the schema of the information you want to extract in an OutputParser. You would then create a PromptTemplate that takes in a raw text blob, with instructions to extract information in the specified format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d760c0-1f4c-4bf0-bbcd-a9f649e371cc",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "This section of documentation covers how we approach and think about evaluation in LangChain. Both evaluation of internal chains/agents, but also how we would recommend people building on top of LangChain approach evaluation.\n",
    "\n",
    "#### The Problem\n",
    "It can be really hard to evaluate LangChain chains and agents. There are two main reasons for this:\n",
    "\n",
    "**1: Lack of data**\n",
    "\n",
    "You generally don't have a ton of data to evaluate your chains/agents over before starting a project. This is usually because Large Language Models (the core of most chains/agents) are terrific few-shot and zero shot learners, meaning you are almost always able to get started on a particular task (text-to-SQL, question answering, etc) without a large dataset of examples. This is in stark contrast to traditional machine learning where you had to first collect a bunch of datapoints before even getting started using a model.\n",
    "\n",
    "**2: Lack of metrics**\n",
    "\n",
    "Most chains/agents are performing tasks for which there are not very good metrics to evaluate performance. For example, one of the most common use cases is generating text of some form. Evaluating generated text is much more complicated than evaluating a classification prediction, or a numeric prediction.\n",
    "\n",
    "#### The Solution\n",
    "LangChain attempts to tackle both of those issues. What we have so far are initial passes at solutions - we do not think we have a perfect solution. So we very much welcome feedback, contributions, integrations, and thoughts on this.\n",
    "\n",
    "Here is what we have for each problem so far:\n",
    "\n",
    "**1: Lack of data**\n",
    "\n",
    "We have started LangChainDatasets a Community space on Hugging Face. We intend this to be a collection of open source datasets for evaluating common chains and agents. We have contributed five datasets of our own to start, but we highly intend this to be a community effort. In order to contribute a dataset, you simply need to join the community and then you will be able to upload datasets.\n",
    "\n",
    "We're also aiming to make it as easy as possible for people to create their own datasets. As a first pass at this, we've added a QAGenerationChain, which given a document comes up with question-answer pairs that can be used to evaluate question-answering tasks over that document down the line.\n",
    "\n",
    "**2: Lack of metrics**\n",
    "\n",
    "We have two solutions to the lack of metrics.\n",
    "\n",
    "The first solution is to use no metrics, and rather just rely on looking at results by eye to get a sense for how the chain/agent is performing. To assist in this, we have developed (and will continue to develop) Tracing, a UI-based visualizer of your chain and agent runs.\n",
    "\n",
    "The second solution we recommend is to use Language Models themselves to evaluate outputs. For this we have a few different chains and prompts aimed at tackling this issue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbe9f29-bae9-486c-aa0b-ecb984154738",
   "metadata": {},
   "source": [
    "#### Summarization\n",
    "A common use case is wanting to summarize long documents. This naturally runs into the context window limitations. Unlike in question-answering, you can't just do some semantic search hacks to only select the chunks of text most relevant to the question (because, in this case, there is no particular question - you want to summarize everything). So what do you do then?\n",
    "\n",
    "The most common way around this is to split the documents into chunks and then do summarization in a recursive manner. By this we mean you first summarize each chunk by itself, then you group the summaries into chunks and summarize each chunk of summaries, and continue doing that until only one is left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84b61cf-0d21-46c6-9c49-6307edb74799",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3723f4a4-8c07-44bf-9e54-d2527ccf81a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a2ecf6-5ef4-487c-8a97-841143471be8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
