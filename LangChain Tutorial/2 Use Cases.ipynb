{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc4930e5-a7d3-418f-ba05-c456df764846",
   "metadata": {},
   "source": [
    "## Use Cases\n",
    "Different end-to-end use cases that LangChain can help with. For each use case, we not only motivate the use case but also discuss which components are most helpful for solving that use case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97faa8a-d53c-4366-9056-331959e0c2fd",
   "metadata": {},
   "source": [
    "### Personal Assistants\n",
    "Personal assistants are a perfect application to build because they combine both of the core value props of LangChain (action taking and personalized data). In order to build a personal assistant you should understand the following concepts:\n",
    "1. `PromptTemplate` - this will guide how your personal assistant acts. Are they sassy? Helpful? These can be used to give your personal assistant some character.\n",
    "2. `Memory` - your personal assistant should probably remember things. They should definitely be able to hold a conversation (short term memory) and they should probably have some concept of long term memory as well.\n",
    "3. `Tools` - your personal assistant will be differentiated by the tools you give it. What should it know how to do?\n",
    "4. `Agent` - your personal assistant will have to understand what actions it should take. Constructing the best agent possible will be important.\n",
    "5. `Agent Executor` - after you've got your tools and your agent, in order to put it into practice you'll need to set up an environment for the agent to use those tools. This is where the Agent Executor comes into play."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d15367-294f-411b-bf1c-4eda00caf88d",
   "metadata": {},
   "source": [
    "### Question Answering Over Documents\n",
    "Although LLMs are powerful, they do not know about information they were not trained on. If you want to use an LLM to answer questions about documents it was not trained on, you have to give it information about those documents. The most common way to do this is through \"retrieval augmented generation\" (RAG).\n",
    "\n",
    "The idea of retrieval augmented generation is that when given a question you first do a retrieval step to fetch any relevant documents. You then pass those documents, along with the original question, to the language model and have it generate a response. In order to do this, however, you first have to have your documents in a format where they can be queried in such a manner. This page goes over the high level ideas between those two steps: (1) ingestion of documents into a queriable format, and then (2) the retrieval augmented generation chain.\n",
    "\n",
    "- **Ingestion** - In order use a language model to interact with your data, you first have to get in a suitable format. That format would be an Index. By putting data into an Index, you make it easy for any downstream steps to interact with it. There are several types of indexes, but by far the most common one is a Vectorstore. Ingesting documents into a vectorstore can be done with the following steps:\n",
    "  - \n",
    "  \n",
    "- **Generation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3723f4a4-8c07-44bf-9e54-d2527ccf81a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a2ecf6-5ef4-487c-8a97-841143471be8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
